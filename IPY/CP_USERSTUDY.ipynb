{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(which, values=1):\n",
    "\n",
    "#     which = 'bars'\n",
    "    # values = 2\n",
    "\n",
    "    RESULTS_DIR = '/n/regal/pfister_lab/PERCEPTION/CP_USERSTUDY/RESULTS/'\n",
    "    \n",
    "    results = [None]*13\n",
    "\n",
    "    with open(RESULTS_DIR + which + '.csv', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    rej_cnt = 0\n",
    "    for l in lines[1:]:\n",
    "        if l.find('Rejected') != -1:\n",
    "            rej_cnt += 1\n",
    "            continue\n",
    "        l_split = l.strip(\"\\r\\n\").split(',')\n",
    "#         print l_split\n",
    "#         print l_split[-1-values]\n",
    "        stimuli = int(l_split[-1 - values].split('/')[-1].split('.')[0])\n",
    "        answer = [int(l_split[-1 -x].strip('\\r\\n').strip('\"')) for x in range(values)]\n",
    "        answer.reverse()\n",
    "\n",
    "        if not results[stimuli]:\n",
    "            results[stimuli] = []\n",
    "        results[stimuli].append(answer)\n",
    "    #         print stimuli, answer\n",
    "\n",
    "#     print 'Rejected', rej_cnt\n",
    "    \n",
    "    return results\n",
    "\n",
    "def merge(a1, a2):\n",
    "    \n",
    "    results = []\n",
    "    for i,a in enumerate(a1):\n",
    "        \n",
    "         results.append(a1[i]+a2[i])\n",
    "            \n",
    "    assert len(results) == 13\n",
    "\n",
    "    return results\n",
    "\n",
    "def grabbygrab(which, values=1, GT_DIR='/n/regal/pfister_lab/PERCEPTION/CP_USERSTUDY/Figure1/'):\n",
    "    \n",
    "    merged = merge(grab(which, values), grab(which+'2', values))\n",
    "#     print merged\n",
    "    if which == 'bars' or which == 'framed':\n",
    "        gt_file = GT_DIR  + '/labels.p' \n",
    "    else:\n",
    "        gt_file = GT_DIR + which + '/labels.p' \n",
    "    with open(gt_file, 'r') as f:\n",
    "        labels = pickle.load(f)\n",
    "        \n",
    "#     print labels\n",
    "    labels = np.array(labels).astype(np.float)\n",
    "    \n",
    "#     if values == 1:\n",
    "    max_label = labels.max()\n",
    "    min_label = labels.min()\n",
    "#     print min_label, max_label\n",
    "#     elif values == 2:\n",
    "#         max_label1 = max(labels)\n",
    "        \n",
    "    if which=='bars' or which=='framed':\n",
    "        labels == labels.ravel()\n",
    "#         min_label = 0\n",
    "        \n",
    "\n",
    "    no_users = len(merged[0])\n",
    "    user_results = []\n",
    "    for u in range(no_users):\n",
    "        \n",
    "        # grab samples for this user\n",
    "        samples = []\n",
    "        for stimuli in merged[3:]:\n",
    "            \n",
    "            user_val = stimuli[u]\n",
    "            \n",
    "            samples.append(user_val)\n",
    "            \n",
    "        if values == 1:\n",
    "            user_results.append(np.array(samples).astype(np.float).ravel())\n",
    "        elif values == 2:\n",
    "            user_results.append(np.array(samples).astype(np.float))\n",
    "        \n",
    "    #\n",
    "    # now normalize our samples\n",
    "    #\n",
    "    \n",
    "    max_labels = {\n",
    "        'position_common_scale': [0,60],\n",
    "        'position_non_aligned_scale': [0,60],\n",
    "        'length': [1, 60],\n",
    "        'direction': [0,359],\n",
    "        'angle': [1,90],\n",
    "        'area': [3.1415, 5026.4],\n",
    "        'volume': [1, 8000],\n",
    "        'curvature': [0,.3],\n",
    "        'shading': [0,100],\n",
    "        'bars': [1,60],\n",
    "        'framed': [1,60],\n",
    "        'weber10': [1,10],\n",
    "        'weber100': [1,10],\n",
    "        'weber1000': [1,10]\n",
    "    }\n",
    "    \n",
    "    min_label = max_labels[which][0]\n",
    "    max_label = max_labels[which][1]\n",
    "    \n",
    "#     min_label = 0\n",
    "#     max_label = 100\n",
    "    for u in range(no_users):\n",
    "        \n",
    "        \n",
    "        user_results[u] -= min_label\n",
    "        user_results[u] /= (max_label - min_label)\n",
    "        \n",
    "    # also normalize the labels\n",
    "    labels -= min_label\n",
    "    labels /= (max_label - min_label)\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    #\n",
    "    # now calculate MLAE\n",
    "    #\n",
    "    MLAES = []\n",
    "    AES = []\n",
    "    for u in range(no_users):\n",
    "        \n",
    "        if which == 'curvature':\n",
    "            user_results[u] /= 100\n",
    "        \n",
    "#         MLAE = sklearn.metrics.mean_absolute_error(user_results[u], labels)#np.log2(sklearn.metrics.mean_absolute_error(user_results[u]/100.*100, labels*100)+.125)\n",
    "        MLAE = np.log2(sklearn.metrics.mean_absolute_error(user_results[u]*100, labels[3:]*100)+.125)\n",
    "        MLAES.append(MLAE)\n",
    "        AE = sklearn.metrics.mean_absolute_error(user_results[u]*100, labels[3:]*100)\n",
    "        AES.append(AE)\n",
    "\n",
    "#     print labels[0:5], user_results[0][0:5]\n",
    "    \n",
    "    \n",
    "        \n",
    "    return MLAES, AES\n",
    "#     print labels, min(labels), max(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position_common_scale\n",
      " MLAE 3.34881979141\n",
      " AE 10.46\n",
      "position_non_aligned_scale\n",
      " MLAE 3.05887139314\n",
      " AE 10.06\n",
      "length\n",
      " MLAE 3.51427088742\n",
      " AE 11.9322033898\n",
      "direction\n",
      " MLAE 3.75379263819\n",
      " AE 13.9788300836\n",
      "angle\n",
      " MLAE 3.21845226441\n",
      " AE 10.197752809\n",
      "area\n",
      " MLAE 3.64010249855\n",
      " AE 12.7397361797\n",
      "volume\n",
      " MLAE 5.17759275094\n",
      " AE 38.0391548944\n",
      "curvature\n",
      " MLAE 4.13489825769\n",
      " AE 17.516\n",
      "shading\n",
      " MLAE 4.21987237194\n",
      " AE 18.2\n"
     ]
    }
   ],
   "source": [
    "which = ['position_common_scale', 'position_non_aligned_scale', 'length', 'direction', 'angle', 'area', 'volume', 'curvature', 'shading']\n",
    "\n",
    "for w in which:\n",
    "    MLAE, AE = grabbygrab(w)\n",
    "    sorted_MLAE = sorted(MLAE)\n",
    "    print w\n",
    "    print ' MLAE', np.mean(sorted_MLAE[6:-6])\n",
    "    print ' AE', np.mean(AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bars\n",
      " MLAE 3.9609493771\n",
      " AE 15.7389830508\n",
      "framed\n",
      " MLAE 3.37105247033\n",
      " AE 11.6542372881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home05/haehn/.conda/envs/CP/lib/python2.7/site-packages/ipykernel_launcher.py:67: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "which = ['bars', 'framed']\n",
    "rect_data = []\n",
    "bar_data = []\n",
    "for w in which:\n",
    "    MLAE, AE = grabbygrab(w,values=2, GT_DIR='/n/regal/pfister_lab/PERCEPTION/CP_USERSTUDY/Figure12/')\n",
    "    sorted_MLAE = sorted(MLAE)\n",
    "    print w\n",
    "    \n",
    "    if w =='bars':\n",
    "        bar_data.append(MLAE)\n",
    "    elif w == 'framed':\n",
    "        rect_data.append(MLAE)\n",
    "    \n",
    "    print ' MLAE', np.mean(sorted_MLAE[6:-6])\n",
    "    print ' AE', np.mean(AE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyvttbl import DataFrame\n",
    "from scipy.stats import ttest_ind, ttest_ind_from_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 3.336 $ ($SD= 0.828 $)\n"
     ]
    }
   ],
   "source": [
    "rect_all_runs = []\n",
    "for r in rect_data:\n",
    "    for e in r:\n",
    "        rect_all_runs.append(e)\n",
    "        \n",
    "print '$',np.round(np.mean(rect_all_runs),3),'$', '($SD=',np.round(np.std(rect_all_runs),3),'$)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ 3.928 $ ($SD= 0.42 $)\n"
     ]
    }
   ],
   "source": [
    "bar_all_runs = []\n",
    "for r in bar_data:\n",
    "    for e in r:\n",
    "        bar_all_runs.append(e)\n",
    "        \n",
    "print '$',np.round(np.mean(bar_all_runs),3),'$', '($SD=',np.round(np.std(bar_all_runs),3),'$)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anova: Single Factor on data\n",
      "\n",
      "SUMMARY\n",
      "Groups   Count    Sum     Average   Variance \n",
      "============================================\n",
      "Bars        25   98.209     3.928      0.184 \n",
      "Rect        25   83.405     3.336      0.714 \n",
      "\n",
      "O'BRIEN TEST FOR HOMOGENEITY OF VARIANCE\n",
      "Source of Variation     SS     df    MS       F     P-value   eta^2   Obs. power \n",
      "================================================================================\n",
      "Treatments             3.509    1   3.509   9.422     0.004   0.164        0.801 \n",
      "Error                 17.875   48   0.372                                        \n",
      "================================================================================\n",
      "Total                 21.384   49                                                \n",
      "\n",
      "ANOVA\n",
      "Source of Variation     SS     df    MS       F     P-value   eta^2   Obs. power \n",
      "================================================================================\n",
      "Treatments             4.384    1   4.384   9.765     0.003   0.169        0.813 \n",
      "Error                 21.548   48   0.449                                        \n",
      "================================================================================\n",
      "Total                 25.931   49                                                \n",
      "\n",
      "POSTHOC MULTIPLE COMPARISONS\n",
      "\n",
      "Tukey HSD: Table of q-statistics\n",
      "       Bars     Rect   \n",
      "======================\n",
      "Bars   0      4.419 ** \n",
      "Rect          0        \n",
      "======================\n",
      "  + p < .10 (q-critical[2, 48] = 2.3719532042)\n",
      "  * p < .05 (q-critical[2, 48] = 2.84346693907)\n",
      " ** p < .01 (q-critical[2, 48] = 3.79320931196)\n"
     ]
    }
   ],
   "source": [
    "# test if the difference between types was significant\n",
    "\n",
    "df = DataFrame()\n",
    "df['data'] = bar_all_runs + rect_all_runs\n",
    "df['conditions'] = ['Bars']*len(bar_all_runs)+['Rect']*len(rect_all_runs)\n",
    "aov_pyvttbl = df.anova1way('data', 'conditions')\n",
    "print aov_pyvttbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weber10\n",
      " MLAE 4.01489242645\n",
      " AE 17.2\n",
      "weber100\n",
      " MLAE 5.38905108448\n",
      " AE 41.9555555556\n",
      "weber1000\n",
      " MLAE 5.46123919279\n",
      " AE 44.3555555556\n"
     ]
    }
   ],
   "source": [
    "which = ['weber10', 'weber100', 'weber1000']\n",
    "\n",
    "for w in which:\n",
    "    MLAE, AE = grabbygrab(w,values=1, GT_DIR='/n/regal/pfister_lab/PERCEPTION/CP_USERSTUDY/Weber/')\n",
    "    sorted_MLAE = sorted(MLAE)\n",
    "    print w\n",
    "    print ' MLAE', np.mean(sorted_MLAE[6:-6])\n",
    "    print ' AE', np.mean(AE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.6402449362223459"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(50-.125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
