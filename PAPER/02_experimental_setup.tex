\section{Experimental Setup}

We conduct quantitative experiments to measure how different convolutional neural networks perceive low-level visual encodings, such as positions, angles, curvatures, and lengths. We formulate these measurement tasks as logistic regression problems: given a stimuli image of an elementary visualization, the networks must estimate the single quantity present or the ratio between multiple quantities present. 

For each experiment, we use a single factor between-subject design, with the factor being the network used. This lets us evaluate whether different network designs are competitive against existing human perception results. We train each network in a supervised fashion with a mean-squared error (MSE) loss between the ground-truth labels and the network's estimate of the measurement from observing the generated stimuli images. Then, we test each network's ability to generalize to new examples with a separate data, created using the same stimuli generator function but with unseen ground-truth measurements (Section~\ref{sec:data}).

% We define a series of hypotheses prior to each experiment.

\subsection{Networks}
\label{sec:networks}

\noindent{\textbf{Multilayer Perceptron.}} As baseline, we use a multilayer perceptron (MLP), but without the prior convolutional layers as is typical in network designs for solving visual tasks (Fig.~\ref{fig:classifiers}). Our MLP contains a layer of $256$ perceptrons, which are activated as rectified linear units (ReLU)~(Fig.~\ref{fig:classifiers}). We train this layer with dropout (probability $= 0.5$) to prevent overfitting, and then combine these ReLU units to regress our output measurement.
\\~\\
\noindent{\textbf{Convolutional Neural Networks.}} We compare different convolutional neural networks (CNNs) with both `trained from scratch' weights and pre-trained weights on a database of natural images (1000-class ImageNet \cite{imagenet}). These networks are the traditional LeNet-5 with 2 layers, which was designed to recognize hand-written digits~\cite{lenet}; the VGG19 network with 19 layers, which was designed to solve the ImageNet object recognition challenge~\cite{simonyan_very_deep2014}; and the Xception network with 36 layers~\cite{xception}, which was also designed to solve the ImageNet object recognition challenge plus the 15,000-class JFT object recognition challenge~\cite{Hinton2015}. Each of these networks has as its last layers an MLP architecture equivalent to our baseline, and so they act as earlier image and feature processors for this final regressor. Since the networks are of different architectures, the number of trainable parameters changes, with some networks having more capacity than others (Table~\ref{tab:parameters}).

For \emph{VGG19} and \emph{Xception}, we have two variants: the network trained from scratch on elementary perceptual tasks, plus the network using weights that were previously trained on the ImageNet object recognition challenge \emph{except} for the MLP layer. This is intended to produce early-layer features which mimic human vision, and then to see whether they are more or less useful than networks trained from scratch. 
\\~\\
\noindent{\textbf{Optimization.}} All network hyperparameters, optimization methods, and stopping conditions are fixed across networks (Table \ref{tab:parameters}). We train for $1000$ epochs using stochastic gradient descent with Nesterov momentum, but stop early if the loss does not decrease for ten epochs.
\\~\\
\noindent{\textbf{Environment.}} We run all experiments on Tesla X and Tesla V100 graphical processing units. We use the KERAS framework with a TensorFlow backend to train the networks, and use the scikit-image library to generate the stimuli. % and scikit-learn libraries - JT: What do we use this for?

\begin{table}[t]
\centering
\caption{\textbf{Network Training.} We use different feature generators as input to a multilayer perceptron which performs linear regression. This results in different sets of trainable parameters. As a baseline, we also train the MLP directly on the visualization images without any additional feature generation.}
\resizebox{\linewidth}{!}{
\begin{tabular}{lrl}
%	\toprule
%	\makecell{Classifier} & \makecell{Convolutional\\Layers} & \makecell{Trainable\\Parameters} \\
%	\midrule
%	MLP & $0$ & $2,560,513$ \\
%	\emph{LeNet} + MLP & $2$ & $8,026,083$ \\
%	\emph{VGG19} + MLP & $16$ & $21,204,545$ \\
%	\emph{Xception} + MLP & $36$ & $25,580,585$ \\
%	\bottomrule
	\toprule
	Network & \makecell{Trainable\\Parameters} & Optimization \\
	\midrule
	MLP & $2,560,513$ & SGD (Nesterov momentum)\\
	\emph{LeNet} + MLP & $8,026,083$ & Learning rate: $0.0001$\\
	\emph{VGG19} + MLP & $21,204,545$ & Momentum: $0.9$ \\
	\emph{Xception} + MLP & $25,580,585$ & \makecell[tl]{Batchsize: 32\\Epochs: $1000$ (Early Stopping)}\\
	\bottomrule
\end{tabular}
}
\label{tab:parameters}
\vspace{-4mm}
\end{table}
%\begin{figure}[t]
%	\centering
%	  \includegraphics[width=\linewidth]{classifiers.pdf}
%  \caption{The multilayer perceptron (MLP) in our experiments has 256 neurons which are activated as rectified linear units (ReLU). We use Dropout regularization to prevent overfitting. We learn categorical and unordered dependent variables using the softmax function and perform linear regression for continuous variables. The MLP can learn the visualizations directly but we also learn features generated by LeNet (2 conv. layers, filter size $5\time5$), VGG19 trained on ImageNet (16 conv. layers, filter size $3\times3$), or Xception trained on ImageNet (36 conv. layers, filter size $3\times3$) to increase the number of trainable parameters.}
%	\label{fig:classifiers}
%\end{figure}
\begin{figure}[t]
	\centering
	
    \subfloat[Feature Generation]{
		\includegraphics[width=4.8cm,valign=c]{classifier_left.pdf}
		\vphantom{\includegraphics[width=3.5cm,valign=c]{classifier_right.pdf}}
	}
	\hfill
    \subfloat[Multilayer Perceptron]{
		\includegraphics[width=3.5cm,valign=c]{classifier_right.pdf}
	}

  \caption{\textbf{Network Architecture.} The multilayer perceptron (MLP) in our experiments has 256 neurons which are activated as rectified linear units (ReLU). We use Dropout regularization to prevent overfitting. As output, we perform linear regression for continuous variables. The MLP can learn to represent the visualizations directly, but we also learn features generated by LeNet (2 conv. layers, filter size $5\times5$), VGG19 (16 conv. layers, filter size $3\times3$), or Xception (36 conv. layers, filter size $3\times3$) to test different model complexities.}
	\label{fig:classifiers}
\end{figure}

\subsection{Data}
\label{sec:data}
We create all visualizations as parametrized rasterized images without interpolation. The number of possible parameter values differs per experiment as summarized in Table~\ref{tab:encoding_parameters} and Section~\ref{sec:parametrizations}. 
\\~\\
\noindent\textbf{Noise.} We add subtle random noise ($0.05$) to each pixel to introduce additional variation. This prevents the networks of learning the actual pixels.
\\~\\
\noindent\textbf{Training/Validation/Test Splits.} We specify the size of each split set as follows: 60,000 training images, 20,000 validation images, and 20,000 test images. Then, we randomly add parameterized visualizations to the sets while guaranteeing that each set is disjunct from each other in terms of encoded variables. This eliminates leakage during training and evaluation. We also scale each set independently: images to the range of $-0.5$ to $0.5$ and labels to the range of $0.0$ to $1.0$. This represents the percent of the value to be measured in the individual experiment. % JT: Moved; but what does this last sentence mean?
\\~\\
\noindent\textbf{Cross Validation.} For reproducibility, we perform repeated random sub-sampling validation, also known as Monte Carlo cross-validation, during our experiments. We run every experiment seperately multiple times (at least 4, up to 12) and randomly select (without replacement) the $60\%$ of our data as training data, $20\%$ as validation, and $20\%$ as test. Our large data sample size of 100,000 guarantees that every single observation of our parameterizations will be selected at least once (excluding noise patterns). Finally, we average the results over the runs.
\\~\\
\noindent\textbf{Intra-classifier Variability.} We also evaluate classifiers previously trained with one visualization on the same type of visualizations with different parameters by decreasing and increasing the variability of the generated images.
\\~\\
\noindent\textbf{Multi Classifiers.} Some experiments compare different types of visual encodings. We first train and evaluate each classifier on one type of encoding but we also train the classifiers on a random selection of multiple different types. These scenarios afflict the optimization process to find global minima and result in networks with more flexible knowledge with the caveat of longer training times.


\subsection{Measures and Analysis}

\noindent{\textbf{Accuracy.}} In their 1984 paper, Cleveland and McGill use the midmean logistic absolute error metric (\emph{MLAE}) to measure perception accuracy.
\begin{equation}
	\textnormal{MLAE} = log_2( | \textnormal{predicted percent} - \textnormal{true percent} | + .125)
\end{equation}
In addition to this metric, we also calculate standard error metrics such as the mean squared error (\emph{MSE}) and the mean absolute error (\emph{MAE}). This allow a more direct comparison of percent errors.
\\~\\
\noindent{\textbf{Efficiency.}} We use the convergence rate based on the decrease of loss per training epoch as an indicator for the efficiency of the classifier in combination with a visual encoding. For regression tasks the loss is defined as MSE.% and for classification tasks the loss is categorical cross-entropy.
\\~\\
\noindent{\textbf{Confidence Intervals.}} We follow the notion of Cleveland and McGill to compute the $95\%$ confidence intervals but rather than performing bootstrapping, we approximate the value of the $97.5$ percentile point of the normal distribution for simplicity.
\\~\\
\noindent{\textbf{Confirmatory Data Analysis.}} To accept or reject our hypotheses, we calculate these measures and treat them as continuous variables. We analyze these dependent variables using analysis of variance (ANOVA) followed by parametric tests.
