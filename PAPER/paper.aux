\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{krizhevsky_imagenet2012,simonyan_very_deep2014,szegedy2015}
\citation{goodfellow_book,deeplearning_blackbox2017}
\citation{yamins2016using,hassabis2017neuroscience}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Here is a fish.}}{1}{figure.1}}
\newlabel{fig:teaser}{{1}{1}{Here is a fish}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Biological Vision (schematic)}}{1}{figure.5}}
\newlabel{fig:vision}{{2}{1}{The Biological Vision (schematic)}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Biological Vision}{1}{subsection.4}}
\citation{cleveland_mcgill}
\citation{HeerBostock2010}
\citation{Wang_linegraph_vs_scatterplot}
\citation{mckenzie_2d_3d}
\citation{forsberg2009comparing_3d_vector}
\citation{laidlaw_2d_vector}
\citation{kindlmann2002color}
\citation{rheingans1992color}
\citation{ware1988color}
\citation{Rogowitz2001_colormaps}
\citation{borkin2011arteries}
\citation{heer2017blackhat}
\citation{herr2009timeseries}
\citation{munzner2015visualization}
\citation{open_vs_closed_shapes}
\citation{harrison2014_webers_law_rank}
\citation{Pineo2012_computational_perception}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\@writefile{toc}{\contentsline {section}{\numberline {2}Previous Work}{2}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Setup}{2}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Measures}{2}{subsection.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Classifiers}{2}{subsection.10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces We use different feature generators as input to a multilayer perceptron which performs linear regression or the classification task. This yields different sets of trainable parameters. We also train the MLP directly on the visualizations without any additional feature generation.}}{2}{table.11}}
\newlabel{tab:parameters}{{1}{2}{We use different feature generators as input to a multilayer perceptron which performs linear regression or the classification task. This yields different sets of trainable parameters. We also train the MLP directly on the visualizations without any additional feature generation}{table.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The multilayer perceptron (MLP) in our experiments has 256 neurons which are activated as rectified linear units (ReLU). We use Dropout regularization to prevent overfitting. We learn categorical and unordered dependent variables using the softmax function and perform linear regression for continuous variables. The MLP can learn the visualizations directly but we also learn features generated by LeNet (2 conv. layers, filter size $5\time 5$), VGG19 trained on ImageNet (16 conv. layers, filter size $3\times 3$), or Xception trained on ImageNet (36 conv. layers, filter size $3\times 3$) to increase the number of trainable parameters.}}{2}{figure.12}}
\newlabel{fig:classifiers}{{3}{2}{The multilayer perceptron (MLP) in our experiments has 256 neurons which are activated as rectified linear units (ReLU). We use Dropout regularization to prevent overfitting. We learn categorical and unordered dependent variables using the softmax function and perform linear regression for continuous variables. The MLP can learn the visualizations directly but we also learn features generated by LeNet (2 conv. layers, filter size $5\time 5$), VGG19 trained on ImageNet (16 conv. layers, filter size $3\times 3$), or Xception trained on ImageNet (36 conv. layers, filter size $3\times 3$) to increase the number of trainable parameters}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Elementary Perceptual Tasks}{2}{section.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parametrization}{2}{subsection.14}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Position-Angle Experiment}{2}{section.15}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Position-Length Experiment}{2}{section.17}}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\bibstyle{abbrv-doi}
\bibdata{paper.bib}
\bibcite{borkin2011arteries}{1}
\bibcite{open_vs_closed_shapes}{2}
\bibcite{cleveland_mcgill}{3}
\bibcite{mckenzie_2d_3d}{4}
\bibcite{heer2017blackhat}{5}
\bibcite{forsberg2009comparing_3d_vector}{6}
\bibcite{goodfellow_book}{7}
\bibcite{harrison2014_webers_law_rank}{8}
\bibcite{hassabis2017neuroscience}{9}
\bibcite{HeerBostock2010}{10}
\bibcite{herr2009timeseries}{11}
\bibcite{kindlmann2002color}{12}
\bibcite{krizhevsky_imagenet2012}{13}
\bibcite{laidlaw_2d_vector}{14}
\bibcite{munzner2015visualization}{15}
\bibcite{Pineo2012_computational_perception}{16}
\bibcite{rheingans1992color}{17}
\bibcite{Rogowitz2001_colormaps}{18}
\bibcite{deeplearning_blackbox2017}{19}
\bibcite{simonyan_very_deep2014}{20}
\bibcite{szegedy2015}{21}
\bibcite{Wang_linegraph_vs_scatterplot}{22}
\bibcite{ware1988color}{23}
\bibcite{yamins2016using}{24}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The Position-Angle Experiment}}{3}{figure.16}}
\newlabel{fig:position_angle_experiment}{{4}{3}{The Position-Angle Experiment}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The Position-Length Experiment}}{3}{figure.18}}
\newlabel{fig:position_length_experiment}{{5}{3}{The Position-Length Experiment}{figure.18}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Bars and Framed Rectangles Experiment}{3}{section.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Weber's Law}{3}{subsection.21}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results and Discussion}{3}{section.22}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusions}{3}{section.26}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The Bars and Framed Rectangles Experiment}}{3}{figure.20}}
\newlabel{fig:bars_and_framed_rectangles_experiment}{{6}{3}{The Bars and Framed Rectangles Experiment}{figure.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Log absolute error means and 95\% confidence intervals for computed perception of different classifiers on the \emph  {elementary perceptual tasks} introduced by Cleveland and McGill 1984\nobreakspace  {}\cite  {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet.}}{4}{figure.23}}
\newlabel{fig:figure1_results}{{7}{4}{Log absolute error means and 95\% confidence intervals for computed perception of different classifiers on the \emph {elementary perceptual tasks} introduced by Cleveland and McGill 1984~\cite {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet}{figure.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Mean Square Error (MSE) loss for the \emph  {position-angle experiment} as described by Cleveland and McGill\nobreakspace  {}\cite  {cleveland_mcgill} which compares the visualization of pie charts and bar charts. We report the MSE measure for both encodings of four different classifier on previously unseen validation data.}}{5}{figure.24}}
\newlabel{fig:position_angle_results}{{8}{5}{Mean Square Error (MSE) loss for the \emph {position-angle experiment} as described by Cleveland and McGill~\cite {cleveland_mcgill} which compares the visualization of pie charts and bar charts. We report the MSE measure for both encodings of four different classifier on previously unseen validation data}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Log absolute error means and 95\% confidence intervals for the \emph  {position-angle experiment} as described by Cleveland and McGill\nobreakspace  {}\cite  {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet.}}{6}{figure.25}}
\newlabel{fig:position_angle_results}{{9}{6}{Log absolute error means and 95\% confidence intervals for the \emph {position-angle experiment} as described by Cleveland and McGill~\cite {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet}{figure.25}{}}
