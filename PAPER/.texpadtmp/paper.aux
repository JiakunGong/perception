\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{krizhevsky_imagenet2012,simonyan_very_deep2014,szegedy2015}
\citation{goodfellow_book,deeplearning_blackbox2017}
\citation{yamins2016using,hassabis2017neuroscience}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Computing Cleveland and McGill's Position-Angle Experiment using Convolutional Neural Networks.} We replicate the original experiment by asking visual cortex inspired machine learning classifiers to assess the relationship between values encoded in pie charts and bar charts. Similar to the findings of Clevenland and McGill\nobreakspace  {}\cite  {cleveland_mcgill}, our experiments show that CNNs predict more accurately when working with bar charts (mean squared error, MSE in green).}}{1}{figure.1}}
\newlabel{fig:teaser}{{1}{1}{\textbf {Computing Cleveland and McGill's Position-Angle Experiment using Convolutional Neural Networks.} We replicate the original experiment by asking visual cortex inspired machine learning classifiers to assess the relationship between values encoded in pie charts and bar charts. Similar to the findings of Clevenland and McGill~\cite {cleveland_mcgill}, our experiments show that CNNs predict more accurately when working with bar charts (mean squared error, MSE in green)}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The Biological Vision (schematic)}}{1}{figure.5}}
\newlabel{fig:vision}{{2}{1}{The Biological Vision (schematic)}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Biological Vision}{1}{subsection.4}}
\citation{cleveland_mcgill}
\citation{HeerBostock2010}
\citation{Wang_linegraph_vs_scatterplot}
\citation{mckenzie_2d_3d}
\citation{forsberg2009comparing_3d_vector}
\citation{laidlaw_2d_vector}
\citation{kindlmann2002color}
\citation{rheingans1992color}
\citation{ware1988color}
\citation{Rogowitz2001_colormaps}
\citation{borkin2011arteries}
\citation{heer2017blackhat}
\citation{herr2009timeseries}
\citation{munzner2015visualization}
\citation{open_vs_closed_shapes}
\citation{harrison2014_webers_law_rank}
\citation{Pineo2012_computational_perception}
\@writefile{toc}{\contentsline {section}{\numberline {2}Previous Work}{2}{section.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Setup}{2}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Measures}{2}{subsection.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Classifiers}{2}{subsection.10}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces We use different feature generators as input to a multilayer perceptron which performs linear regression or the classification task. This yields different sets of trainable parameters. We also train the MLP directly on the visualizations without any additional feature generation.}}{2}{table.11}}
\newlabel{tab:parameters}{{1}{2}{We use different feature generators as input to a multilayer perceptron which performs linear regression or the classification task. This yields different sets of trainable parameters. We also train the MLP directly on the visualizations without any additional feature generation}{table.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The multilayer perceptron (MLP) in our experiments has 256 neurons which are activated as rectified linear units (ReLU). We use Dropout regularization to prevent overfitting. We learn categorical and unordered dependent variables using the softmax function and perform linear regression for continuous variables. The MLP can learn the visualizations directly but we also learn features generated by LeNet (2 conv. layers, filter size $5\time 5$), VGG19 trained on ImageNet (16 conv. layers, filter size $3\times 3$), or Xception trained on ImageNet (36 conv. layers, filter size $3\times 3$) to increase the number of trainable parameters.}}{2}{figure.12}}
\newlabel{fig:classifiers}{{3}{2}{The multilayer perceptron (MLP) in our experiments has 256 neurons which are activated as rectified linear units (ReLU). We use Dropout regularization to prevent overfitting. We learn categorical and unordered dependent variables using the softmax function and perform linear regression for continuous variables. The MLP can learn the visualizations directly but we also learn features generated by LeNet (2 conv. layers, filter size $5\time 5$), VGG19 trained on ImageNet (16 conv. layers, filter size $3\times 3$), or Xception trained on ImageNet (36 conv. layers, filter size $3\times 3$) to increase the number of trainable parameters}{figure.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Data}{2}{subsection.13}}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Elementary Perceptual Tasks.} Rasterized visualizations of the elementary perceptual tasks as defined by Cleveland and McGill\nobreakspace  {}\cite  {cleveland_mcgill} (color saturation excluded). We vary the parameters of each perceptual task and then assess the interpretability of feed-forward neural networks.}}{3}{figure.16}}
\newlabel{fig:elementary_perceptual_tasks}{{4}{3}{\textbf {Elementary Perceptual Tasks.} Rasterized visualizations of the elementary perceptual tasks as defined by Cleveland and McGill~\cite {cleveland_mcgill} (color saturation excluded). We vary the parameters of each perceptual task and then assess the interpretability of feed-forward neural networks}{figure.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Elementary Perceptual Tasks}{3}{section.14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Parametrizations}{3}{subsection.15}}
\newlabel{sec:parametrizations}{{4.1}{3}{Parametrizations}{subsection.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Hypotheses}{3}{subsection.18}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Variability of Elementary Perceptual Tasks.} We sequentially increase the number of parameters for every visual encoding of the elementary perceptual tasks. This introduces variability and increasingly more complex datasets.}}{3}{table.17}}
\newlabel{tab:encoding_parameters}{{2}{3}{\textbf {Variability of Elementary Perceptual Tasks.} We sequentially increase the number of parameters for every visual encoding of the elementary perceptual tasks. This introduces variability and increasingly more complex datasets}{table.17}{}}
\citation{cleveland_mcgill}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Position-Angle Experiment.} We create rasterized visualizations of pie charts and bar charts to follow Cleveland and McGill's position-angle experiment. The experimental task involves the judgement of different encoded values in comparison to the largest encoded values. The pie chart (left) and the bar chart (right) visualize the same data point. In their paper, Cleveland and McGill report less errors using bar charts.}}{4}{figure.20}}
\newlabel{fig:position_angle_experiment}{{5}{4}{\textbf {Position-Angle Experiment.} We create rasterized visualizations of pie charts and bar charts to follow Cleveland and McGill's position-angle experiment. The experimental task involves the judgement of different encoded values in comparison to the largest encoded values. The pie chart (left) and the bar chart (right) visualize the same data point. In their paper, Cleveland and McGill report less errors using bar charts}{figure.20}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Position-Angle Experiment}{4}{section.19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Hypotheses}{4}{subsection.21}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Position-Length Experiment}{4}{section.22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Hypotheses}{4}{subsection.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Position-Length Experiment.} (Not yet) Rasterized versions of the graphs of Cleveland and McGill's position-length experiment. The perceptual task involves comparing. the two dot-marked quantities across five different visual encodings of either grouped or divided bar charts. We evaluate which type of bar chart performs better with our neural networks as a combined classification and regression problem. The first task is to select which of the marked quantities is smaller (classification) and the second task is to specify how much smaller it is (regression).}}{4}{figure.24}}
\newlabel{fig:position_length_experiment}{{6}{4}{\textbf {Position-Length Experiment.} (Not yet) Rasterized versions of the graphs of Cleveland and McGill's position-length experiment. The perceptual task involves comparing. the two dot-marked quantities across five different visual encodings of either grouped or divided bar charts. We evaluate which type of bar chart performs better with our neural networks as a combined classification and regression problem. The first task is to select which of the marked quantities is smaller (classification) and the second task is to specify how much smaller it is (regression)}{figure.24}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Bars and Framed Rectangles Experiment}{4}{section.25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Hypotheses}{4}{subsection.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Weber-Fechner's Law}{4}{subsection.28}}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\citation{cleveland_mcgill}
\bibstyle{abbrv-doi}
\bibdata{paper.bib}
\bibcite{borkin2011arteries}{1}
\bibcite{open_vs_closed_shapes}{2}
\bibcite{cleveland_mcgill}{3}
\bibcite{mckenzie_2d_3d}{4}
\bibcite{heer2017blackhat}{5}
\bibcite{forsberg2009comparing_3d_vector}{6}
\bibcite{goodfellow_book}{7}
\bibcite{harrison2014_webers_law_rank}{8}
\bibcite{hassabis2017neuroscience}{9}
\bibcite{HeerBostock2010}{10}
\bibcite{herr2009timeseries}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Bars and Framed Rectangles Experiment.} Cleveland and McGill introduce the bars and framed rectangles experiment which measures the perceptual task of judging position along non-aligned scales. For humans, it is easier to decide which of two bars represent a larger height if a scale is introduced by adding framed rectangles (right). In this case, the right bar is heigher as visible with less free space when adding the frame. We evaluate whether such a visual aid also helps machines to perceive visually encoded quantities.}}{5}{figure.27}}
\newlabel{fig:bars_and_framed_rectangles_experiment}{{7}{5}{\textbf {Bars and Framed Rectangles Experiment.} Cleveland and McGill introduce the bars and framed rectangles experiment which measures the perceptual task of judging position along non-aligned scales. For humans, it is easier to decide which of two bars represent a larger height if a scale is introduced by adding framed rectangles (right). In this case, the right bar is heigher as visible with less free space when adding the frame. We evaluate whether such a visual aid also helps machines to perceive visually encoded quantities}{figure.27}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results and Discussion}{5}{section.30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Elementary Perceptual Tasks}{5}{subsection.31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Position-Angle Experiment}{5}{subsection.40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Position-Length Experiment}{5}{subsection.43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Bars and Framed Rectangles Experiment}{5}{subsection.44}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusions}{5}{section.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \textbf  {Weber-Fechner Law.} The Weber-Fechner law states that the perceivable differences within a distribution is proportional to the initial size of the distribution. The lower square contains 10 more dots than the upper one on both sides. However, the difference is easily perceivable on the left while the squares on the right almost look the same. We generate rasterized visualizations similar to this setup and evaluate our classifiers.}}{5}{figure.29}}
\newlabel{fig:webers_law}{{8}{5}{\textbf {Weber-Fechner Law.} The Weber-Fechner law states that the perceivable differences within a distribution is proportional to the initial size of the distribution. The lower square contains 10 more dots than the upper one on both sides. However, the difference is easily perceivable on the left while the squares on the right almost look the same. We generate rasterized visualizations similar to this setup and evaluate our classifiers}{figure.29}{}}
\bibcite{kindlmann2002color}{12}
\bibcite{krizhevsky_imagenet2012}{13}
\bibcite{laidlaw_2d_vector}{14}
\bibcite{munzner2015visualization}{15}
\bibcite{Pineo2012_computational_perception}{16}
\bibcite{rheingans1992color}{17}
\bibcite{Rogowitz2001_colormaps}{18}
\bibcite{deeplearning_blackbox2017}{19}
\bibcite{simonyan_very_deep2014}{20}
\bibcite{szegedy2015}{21}
\bibcite{Wang_linegraph_vs_scatterplot}{22}
\bibcite{ware1988color}{23}
\bibcite{yamins2016using}{24}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \textbf  {Computational results of Elementary Perceptual Tasks experiment.} Log absolute error means and 95\% confidence intervals for computed perception of different classifiers on the \emph  {elementary perceptual tasks} introduced by Cleveland and McGill 1984\nobreakspace  {}\cite  {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet.}}{7}{figure.32}}
\newlabel{fig:figure1_results}{{9}{7}{\textbf {Computational results of Elementary Perceptual Tasks experiment.} Log absolute error means and 95\% confidence intervals for computed perception of different classifiers on the \emph {elementary perceptual tasks} introduced by Cleveland and McGill 1984~\cite {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet}{figure.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \textbf  {Classifier Efficiency of the Position-Angle experiment.} Mean Square Error (MSE) loss for the \emph  {position-angle experiment} as described by Cleveland and McGill\nobreakspace  {}\cite  {cleveland_mcgill} which compares the visualization of pie charts and bar charts. We report the MSE measure for both encodings of four different classifier on previously unseen validation data.}}{8}{figure.41}}
\newlabel{fig:figure3_val_loss}{{10}{8}{\textbf {Classifier Efficiency of the Position-Angle experiment.} Mean Square Error (MSE) loss for the \emph {position-angle experiment} as described by Cleveland and McGill~\cite {cleveland_mcgill} which compares the visualization of pie charts and bar charts. We report the MSE measure for both encodings of four different classifier on previously unseen validation data}{figure.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \textbf  {Computational results of the Position-Angle experiment.} Log absolute error means and 95\% confidence intervals for the \emph  {position-angle experiment} as described by Cleveland and McGill\nobreakspace  {}\cite  {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet.}}{9}{figure.42}}
\newlabel{fig:figure3_mlae}{{11}{9}{\textbf {Computational results of the Position-Angle experiment.} Log absolute error means and 95\% confidence intervals for the \emph {position-angle experiment} as described by Cleveland and McGill~\cite {cleveland_mcgill}. We test the performance of a Multi-layer Perceptron (MLP), the LeNet Convolutional Neural Network, as well as feature generation using the VGG19 and Xception networks trained on ImageNet}{figure.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \textbf  {Classifier Efficiency of the Bars and Framed Rectangles experiment.} Categorical Cross-Entropy loss for the \emph  {bars and framed rectangles experiment} as described by Cleveland and McGill\nobreakspace  {}\cite  {cleveland_mcgill}. The frame around the bars adds an additional visual cue enables faster network convergence. This is not yet reproducible!}}{9}{figure.45}}
\newlabel{fig:figure12_val_loss}{{12}{9}{\textbf {Classifier Efficiency of the Bars and Framed Rectangles experiment.} Categorical Cross-Entropy loss for the \emph {bars and framed rectangles experiment} as described by Cleveland and McGill~\cite {cleveland_mcgill}. The frame around the bars adds an additional visual cue enables faster network convergence. This is not yet reproducible!}{figure.45}{}}
