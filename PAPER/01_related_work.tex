\section{Related Work}

\noindent{\textbf{Graphical Perception.}} Cleveland and McGill~\cite{cleveland_mcgill,cleveland1985graphical} coined the phrase \emph{graphical perception} \change{to describe ``the visual decoding of information encoded on graphs''.} To understand encodings in visualizations, they define \emph{elementary perceptual tasks} as mental-visual stimuli, and rank their perceptual difficulty \change{to humans}. From these definitions, the authors perform the \emph{position-angle} experiment to compare bar charts and pie charts, and the \emph{position-length} experiment where users judge relations between encoded quantities in grouped and divided bar charts. The authors then use this to redesign a statistical map via \emph{bars and framed rectangles} and Weber's law \cite{harrison2014_webers_law_rank}, using the proportional relation between an initial distribution density and perceivable change.

Heer and Bostock later reproduced the Cleveland-McGill experiments via crowd-sourcing on Mechanical Turk~\cite{HeerBostock2010}, with similar results. Harrison et al.~repeated the Cleveland-McGill experiments while observing viewer emotional states, again with similar results~\cite{harrison2013influencing}. \change{Talbot et al.~delve deeper into the impact of specific bar chart design variations on human prediction \cite{Talbot2014}}. While we focus on Cleveland and McGill's work due to its repeated reproduction, many works investigate human perception to visual encoding~\cite{mackinlay1988applying,bertin1967semiologie,treisman1988feature,carpendale2003considering,wilkinson2006grammar,widgor_perception2007,munzner2015visualization}.

\change{
Cleveland and McGill's definition of graphical perception judiciously excludes a human subject and a visual decoding method, leaving the door open for machine perception such as with CNNs. Their quality standard is only that the machine must decode \emph{information}, which leaves narrow and well-defined applications like our experiments approachable. However, machine graphical perception must meet human breadth in capability to be generally useful in a human world.
}
%
%Interesting are also the rankings of correlation visualization using Weber's law~\cite{harrison2014_webers_law_rank}. This law defines the proportional relation between the initial distribution density and perceivable change. In this paper, we investigate with a simple experiment whether this holds for convolutional neural networks
%
%\textbf{Comparing Visual Encodings.} Different visual encodings have advantages or disadvantages and the community does a great job comparing them. Higher-level comparisons include 2D versus 3D vector field and rendering studies~\cite{mckenzie_2d_3d,forsberg2009comparing_3d_vector,laidlaw_2d_vector,borkin2011arteries}, timeseries~\cite{herr2009timeseries} and scatterplots~\cite{tremmel1995visual,Wang_linegraph_vs_scatterplot}. Lower-level experiments target - besides others - open versus closed encodings~\cite{open_vs_closed_shapes}, and several evaluations of color space~\cite{ware1988color,rheingans1992color,Rogowitz2001_colormaps,kindlmann2002color}. While we investigate lower-level visual encodings in this work, we delay colorspace experiments for future work
%
%
%CNNs trained on natural images have low-level `neurons' which activate on similar stimuli to early neurons in the human visual cortex, e.g., Gabor filter-like stimuli \misscite.
\\~\\
\noindent{\textbf{Visual-cortex-inspired Machine Learning.}} The human visual cortex allows us to recognize objects in the world seemingly without effort. This visual system is organized into layers, which inspired computational systems based on multilayer neural networks. Fukushima and Miyake developed the early Neocognitron quantitative model~\cite{fukushima1982neocognitron}, which ultimately led to the work of Hinton, Bengio, and LeCun~\cite{lecun2015deep} and today's GPU-powered deep neural networks. Such networks have been developed with many architectures \change{attempting to model properties useful to visual reasoning, like translation invariance or part hierarchies}. \change{Across these, the error behaviors of CNNs indicate that they process images in a different way to humans even though they are loosely biologically inspired~\cite{SzegedyICLR2014,Engstrom2017,Azulay2018}.} 
%
%For this paper, we compare a set of networks with different architectures and depths, plus networks with weights trained on natural images and on generated stimuli of the elementary perceptual reasoning tasks.
%
%Object detection is extremely difficult and therefore is especially impressive as light intensities can change by levels of magnitude and contrast between foreground and background is so often low. 
%In addition, the visual scene changes every time the human body or human eyes move. 
%This visual system exhibits a very noisy structure but because it is organized by layers it has inspired the mathematical theory of multilayer neural networks. 
%
%In 1962 Hubel and Wiesel were the first to begin studying the visual cortex from the standpoint of a neuroscientist. Their experimental findings on cats and macaque monkeys suggested a hierarchy of cells with increasing complexity which was then later transferred to the hierarchical model of different layers. Twenty years later, this insight was translated to the Neocognitron quantitative model, by Fukushima and Miyake, which ultimately led to the important work of Hinton, Bengio, and LeCun in the 1980s. Their work on stochastic gradient descent approximation, and the availability of faster computer hardware then led to todayâ€™s deep learning networks. In the last decade, this field has exhibited rapid growth, constant evolution, and new applications in various domains.
%
%The reported classification performance is superior to that of humans and the question in regards to their functionality opens an interesting research topic.
%Visual cortex inspired machine learning classifiers exist with many different architectures. We select the traditional \emph{LeNet-5}~\cite{lenet} which was designed to recognize hand-written digits, the VGG19~\cite{simonyan_very_deep2014} classifier with 16 convolutional layers, and the Xception~\cite{xception} classifier with 36 convolutional layers. 
%
%What is remarkable is that even though current machine learning models do not resemble the complexity of its biological pendant, they inherently are able to generalize but only after learning thousands of examples. 
%Neural networks trained on one specific task can be used to perform detection or segmentation of, seemingly, unrelated objects with relatively minor retraining~\cite{transfer_learning}. 
%
%
%We think that the biological inspiration of modern convolutional neural networks yields the evaluation of principles of human perception with computers.
%
%
%MLP, LeNet, VGG, ImageNet, XCeption, ResNets etc and work from THomas Serre
%
%
%- https://link.springer.com/chapter/10.1007/978-3-642-14600-8_46
%
%- https://github.com/tidyverse/ggplot2/wiki/Recommended-Reading
%
%- last year's VADL workshop: https://vadl2017.github.io/
%
%
\\~\\
\noindent{\textbf{Computational Visualization Analysis.}} Pineo et al.~create a computational model of early human vision based on neural networks, then optimize flow visualizations for comprehension~\cite{Pineo2012_computational_perception}. Their simulations show that visualization perception triggers neural activity in higher-level areas of cognition, which the authors suspect is supported by low-level neurons performing elementary perceptional tasks. Other work tries to parse infographics by finding higher-level saliency models~\cite{bylinskii2016should,bylinskii2017understanding,bylinskii2017predimportance}, \change{or tries to parse text and key visual elements from visualizations using both classic and deep-learning-based computer vision~\cite{Savva2011-revision,kembhavi2016diagram,zoya_text_visual_tags,maneesh_deconstructing_d3,Poco2017-reverse-engineering-vis} (to name but a few)}.

%Poco et al.~recover visual encodings from chart images by \cite{Poco2017-reverse-engineering-vis}
%
%Savva et al.~similarly recover chart elements by \cite{Savva2011-revision}.

%R. A. Al-Zaidy and C. L. Giles. Automatic extraction of datafrombarcharts. InProceedings of the 8th International Conference on Knowledge Capture, page 30. ACM, 2015. 2

\change{
The field of visual question answering (VQA) has begun to combine image and text feature vectors for visualizations. Kafle et al.~extend stacked attention networks with dynamic encodings to support different bar chart designs, plus present a benchmark dataset~\cite{kafle2018dvqa}. However, the ability of CNNs to solve visual relation problems like VQA has been called into question~\cite{not_so_clevr}. Kahou et al.~attempt to answer visual questions across five chart types. However, the authors state that the task poses ``a significant machine learning challenge'' with no tested system able to meet human-level performance~\cite{Kahou2018}.
}

\change{Our work takes a step back. None of these works investigate the building blocks of visualization: elementary perceptual tasks such as position, length, and angle estimation. To produce useful visualization analyzers, we must be able to computationally predict human responses in these tasks. We investigate whether this is possible with CNNs.}

%Feed forward neural networks - visual relations strain
