\relax 
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Experiment: Anti-aliasing}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Anti-aliasing.} We test whether anti-aliasing effects the performance of our networks on pie charts by measuring MLAE. The difference is not statistically significant ($F(1,30)=0.341,p>0.5$).\relax }}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:aa}{{1}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Elementary perceptual tasks.} Midmean logistic absolute errors (MLAE) for all generated stimuli and across all networks. The * indicates networks which use ImageNet weights instead of bein trained from scratch.\relax }}{2}}
\newlabel{fig:epc_mlae}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Elementary perceptual tasks.} Midmean logistic absolute errors (MLAE) visualized as box plots.\relax }}{3}}
\newlabel{fig:epc_mlae_boxplots}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Cross-network variability.} Our networks fail when the stimuli changes through translation or stroke width. The x-labels indicate the training configuration while the y-labels indicate the stimuli variation. Numbers represent MLAE.\relax }}{4}}
\newlabel{fig:cross_network}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Error distributions.} Error distributions of our networks when decoding elementary perceptual tasks.\relax }}{5}}
\newlabel{fig:errors}{{5}{5}}
